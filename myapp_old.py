#Bismillah

LANGUAGES = {
    'af': 'afrikaans',
    'sq': 'albanian',
    'am': 'amharic',
    'ar': 'arabic',
    'hy': 'armenian',
    'az': 'azerbaijani',
    'eu': 'basque',
    'be': 'belarusian',
    'bn': 'bengali',
    'bs': 'bosnian',
    'bg': 'bulgarian',
    'ca': 'catalan',
    'ceb': 'cebuano',
    'ny': 'chichewa',
    'zh-cn': 'chinese (simplified)',
    'zh-tw': 'chinese (traditional)',
    'co': 'corsican',
    'hr': 'croatian',
    'cs': 'czech',
    'da': 'danish',
    'nl': 'dutch',
    'en': 'english',
    'eo': 'esperanto',
    'et': 'estonian',
    'tl': 'filipino',
    'fi': 'finnish',
    'fr': 'french',
    'fy': 'frisian',
    'gl': 'galician',
    'ka': 'georgian',
    'de': 'german',
    'el': 'greek',
    'gu': 'gujarati',
    'ht': 'haitian creole',
    'ha': 'hausa',
    'haw': 'hawaiian',
    'iw': 'hebrew',
    'he': 'hebrew',
    'hi': 'hindi',
    'hmn': 'hmong',
    'hu': 'hungarian',
    'is': 'icelandic',
    'ig': 'igbo',
    'id': 'indonesian',
    'ga': 'irish',
    'it': 'italian',
    'ja': 'japanese',
    'jw': 'javanese',
    'kn': 'kannada',
    'kk': 'kazakh',
    'km': 'khmer',
    'ko': 'korean',
    'ku': 'kurdish (kurmanji)',
    'ky': 'kyrgyz',
    'lo': 'lao',
    'la': 'latin',
    'lv': 'latvian',
    'lt': 'lithuanian',
    'lb': 'luxembourgish',
    'mk': 'macedonian',
    'mg': 'malagasy',
    'ms': 'malay',
    'ml': 'malayalam',
    'mt': 'maltese',
    'mi': 'maori',
    'mr': 'marathi',
    'mn': 'mongolian',
    'my': 'myanmar (burmese)',
    'ne': 'nepali',
    'no': 'norwegian',
    'or': 'odia',
    'ps': 'pashto',
    'fa': 'persian',
    'pl': 'polish',
    'pt': 'portuguese',
    'pa': 'punjabi',
    'ro': 'romanian',
    'ru': 'russian',
    'sm': 'samoan',
    'gd': 'scots gaelic',
    'sr': 'serbian',
    'st': 'sesotho',
    'sn': 'shona',
    'sd': 'sindhi',
    'si': 'sinhala',
    'sk': 'slovak',
    'sl': 'slovenian',
    'so': 'somali',
    'es': 'spanish',
    'su': 'sundanese',
    'sw': 'swahili',
    'sv': 'swedish',
    'tg': 'tajik',
    'ta': 'tamil',
    'tt': 'tatar',
    'te': 'telugu',
    'th': 'thai',
    'tr': 'turkish',
    'tk': 'turkmen',
    'uk': 'ukrainian',
    'ur': 'urdu',
    'ug': 'uyghur',
    'uz': 'uzbek',
    'vi': 'vietnamese',
    'cy': 'welsh',
    'xh': 'xhosa',
    'yi': 'yiddish',
    'yo': 'yoruba',
    'zu': 'zulu',
}

DEFAULT_SERVICE_URLS = ('translate.google.ac','translate.google.ad','translate.google.ae',
                        'translate.google.al','translate.google.am','translate.google.as',
                        'translate.google.at','translate.google.az','translate.google.ba',
                        'translate.google.be','translate.google.bf','translate.google.bg',
                        'translate.google.bi','translate.google.bj','translate.google.bs',
                        'translate.google.bt','translate.google.by','translate.google.ca',
                        'translate.google.cat','translate.google.cc','translate.google.cd',
                        'translate.google.cf','translate.google.cg','translate.google.ch',
                        'translate.google.ci','translate.google.cl','translate.google.cm',
                        'translate.google.cn','translate.google.co.ao','translate.google.co.bw',
                        'translate.google.co.ck','translate.google.co.cr','translate.google.co.id',
                        'translate.google.co.il','translate.google.co.in','translate.google.co.jp',
                        'translate.google.co.ke','translate.google.co.kr','translate.google.co.ls',
                        'translate.google.co.ma','translate.google.co.mz','translate.google.co.nz',
                        'translate.google.co.th','translate.google.co.tz','translate.google.co.ug',
                        'translate.google.co.uk','translate.google.co.uz','translate.google.co.ve',
                        'translate.google.co.vi','translate.google.co.za','translate.google.co.zm',
                        'translate.google.co.zw','translate.google.co','translate.google.com.af',
                        'translate.google.com.ag','translate.google.com.ai','translate.google.com.ar',
                        'translate.google.com.au','translate.google.com.bd','translate.google.com.bh',
                        'translate.google.com.bn','translate.google.com.bo','translate.google.com.br',
                        'translate.google.com.bz','translate.google.com.co','translate.google.com.cu',
                        'translate.google.com.cy','translate.google.com.do','translate.google.com.ec',
                        'translate.google.com.eg','translate.google.com.et','translate.google.com.fj',
                        'translate.google.com.gh','translate.google.com.gi','translate.google.com.gt',
                        'translate.google.com.hk','translate.google.com.jm','translate.google.com.kh',
                        'translate.google.com.kw','translate.google.com.lb','translate.google.com.lc',
                        'translate.google.com.ly','translate.google.com.mm','translate.google.com.mt',
                        'translate.google.com.mx','translate.google.com.my','translate.google.com.na',
                        'translate.google.com.ng','translate.google.com.ni','translate.google.com.np',
                        'translate.google.com.om','translate.google.com.pa','translate.google.com.pe',
                        'translate.google.com.pg','translate.google.com.ph','translate.google.com.pk',
                        'translate.google.com.pr','translate.google.com.py','translate.google.com.qa',
                        'translate.google.com.sa','translate.google.com.sb','translate.google.com.sg',
                        'translate.google.com.sl','translate.google.com.sv','translate.google.com.tj',
                        'translate.google.com.tr','translate.google.com.tw','translate.google.com.ua',
                        'translate.google.com.uy','translate.google.com.vc','translate.google.com.vn',
                        'translate.google.com','translate.google.cv','translate.google.cx',
                        'translate.google.cz','translate.google.de','translate.google.dj',
                        'translate.google.dk','translate.google.dm','translate.google.dz',
                        'translate.google.ee','translate.google.es','translate.google.eu',
                        'translate.google.fi','translate.google.fm','translate.google.fr',
                        'translate.google.ga','translate.google.ge','translate.google.gf',
                        'translate.google.gg','translate.google.gl','translate.google.gm',
                        'translate.google.gp','translate.google.gr','translate.google.gy',
                        'translate.google.hn','translate.google.hr','translate.google.ht',
                        'translate.google.hu','translate.google.ie','translate.google.im',
                        'translate.google.io','translate.google.iq','translate.google.is',
                        'translate.google.it','translate.google.je','translate.google.jo',
                        'translate.google.kg','translate.google.ki','translate.google.kz',
                        'translate.google.la','translate.google.li','translate.google.lk',
                        'translate.google.lt','translate.google.lu','translate.google.lv',
                        'translate.google.md','translate.google.me','translate.google.mg',
                        'translate.google.mk','translate.google.ml','translate.google.mn',
                        'translate.google.ms','translate.google.mu','translate.google.mv',
                        'translate.google.mw','translate.google.ne','translate.google.nf',
                        'translate.google.nl','translate.google.no','translate.google.nr',
                        'translate.google.nu','translate.google.pl','translate.google.pn',
                        'translate.google.ps','translate.google.pt','translate.google.ro',
                        'translate.google.rs','translate.google.ru','translate.google.rw',
                        'translate.google.sc','translate.google.se','translate.google.sh',
                        'translate.google.si','translate.google.sk','translate.google.sm',
                        'translate.google.sn','translate.google.so','translate.google.sr',
                        'translate.google.st','translate.google.td','translate.google.tg',
                        'translate.google.tk','translate.google.tl','translate.google.tm',
                        'translate.google.tn','translate.google.to','translate.google.tt',
                        'translate.google.us','translate.google.vg','translate.google.vu','translate.google.ws')


# coding:utf-8
# author LuShan
# version : 1.1.9
import json, requests, random, re
from urllib.parse import quote
import urllib3
import logging


log = logging.getLogger(__name__)
log.addHandler(logging.NullHandler())

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

URLS_SUFFIX = [re.search('translate.google.(.*)', url.strip()).group(1) for url in DEFAULT_SERVICE_URLS]
URL_SUFFIX_DEFAULT = 'cn'


class google_new_transError(Exception):
    """Exception that uses context to present a meaningful error message"""

    def __init__(self, msg=None, **kwargs):
        self.tts = kwargs.pop('tts', None)
        self.rsp = kwargs.pop('response', None)
        if msg:
            self.msg = msg
        elif self.tts is not None:
            self.msg = self.infer_msg(self.tts, self.rsp)
        else:
            self.msg = None
        super(google_new_transError, self).__init__(self.msg)

    def infer_msg(self, tts, rsp=None):
        cause = "Unknown"

        if rsp is None:
            premise = "Failed to connect"

            return "{}. Probable cause: {}".format(premise, "timeout")
            # if tts.tld != 'com':
            #     host = _translate_url(tld=tts.tld)
            #     cause = "Host '{}' is not reachable".format(host)

        else:
            status = rsp.status_code
            reason = rsp.reason

            premise = "{:d} ({}) from TTS API".format(status, reason)

            if status == 403:
                cause = "Bad token or upstream API changes"
            elif status == 200 and not tts.lang_check:
                cause = "No audio stream in response. Unsupported language '%s'" % self.tts.lang
            elif status >= 500:
                cause = "Uptream API error. Try again later."

        return "{}. Probable cause: {}".format(premise, cause)


class google_translator:
    '''
    You can use 108 language in target and source,details view LANGUAGES.
    Target language: like 'en'、'zh'、'th'...

    :param url_suffix: The source text(s) to be translated. Batch translation is supported via sequence input.
                       The value should be one of the url_suffix listed in : `DEFAULT_SERVICE_URLS`
    :type url_suffix: UTF-8 :class:`str`; :class:`unicode`; string sequence (list, tuple, iterator, generator)

    :param text: The source text(s) to be translated.
    :type text: UTF-8 :class:`str`; :class:`unicode`;

    :param lang_tgt: The language to translate the source text into.
                     The value should be one of the language codes listed in : `LANGUAGES`
    :type lang_tgt: :class:`str`; :class:`unicode`

    :param lang_src: The language of the source text.
                    The value should be one of the language codes listed in :const:`googletrans.LANGUAGES`
                    If a language is not specified,
                    the system will attempt to identify the source language automatically.
    :type lang_src: :class:`str`; :class:`unicode`

    :param timeout: Timeout Will be used for every request.
    :type timeout: number or a double of numbers

    :param proxies: proxies Will be used for every request.
    :type proxies: class : dict; like: {'http': 'http:171.112.169.47:19934/', 'https': 'https:171.112.169.47:19934/'}

    '''

    def __init__(self, url_suffix="cn", timeout=5, proxies=None):
        self.proxies = proxies
        if url_suffix not in URLS_SUFFIX:
            self.url_suffix = URL_SUFFIX_DEFAULT
        else:
            self.url_suffix = url_suffix
        url_base = "https://translate.google.{}".format(self.url_suffix)
        self.url = url_base + "/_/TranslateWebserverUi/data/batchexecute"
        self.timeout = timeout

    def _package_rpc(self, text, lang_src='auto', lang_tgt='auto'):
        GOOGLE_TTS_RPC = ["MkEWBc"]
        parameter = [[text.strip(), lang_src, lang_tgt, True], [1]]
        escaped_parameter = json.dumps(parameter, separators=(',', ':'))
        rpc = [[[random.choice(GOOGLE_TTS_RPC), escaped_parameter, None, "generic"]]]
        espaced_rpc = json.dumps(rpc, separators=(',', ':'))
        # text_urldecode = quote(text.strip())
        freq_initial = "f.req={}&".format(quote(espaced_rpc))
        freq = freq_initial
        return freq

    def translate(self, text, lang_tgt='auto', lang_src='auto', pronounce=False):
        try:
            lang = LANGUAGES[lang_src]
        except:
            lang_src = 'auto'
        try:
            lang = LANGUAGES[lang_tgt]
        except:
            lang_src = 'auto'
        text = str(text)
        if len(text) >= 5000:
            return "Warning: Can only detect less than 5000 characters"
        if len(text) == 0:
            return ""
        headers = {
            "Referer": "http://translate.google.{}/".format(self.url_suffix),
            "User-Agent":
                "Mozilla/5.0 (Windows NT 10.0; WOW64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/47.0.2526.106 Safari/537.36",
            "Content-Type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        freq = self._package_rpc(text, lang_src, lang_tgt)
        response = requests.Request(method='POST',
                                    url=self.url,
                                    data=freq,
                                    headers=headers,
                                    )
        try:
            if self.proxies == None or type(self.proxies) != dict:
                self.proxies = {}
            with requests.Session() as s:
                s.proxies = self.proxies
                r = s.send(request=response.prepare(),
                           verify=False,
                           timeout=self.timeout)
            for line in r.iter_lines(chunk_size=1024):
                decoded_line = line.decode('utf-8')
                if "MkEWBc" in decoded_line:
                    try:
                        response = (decoded_line)
                        response = json.loads(response)
                        response = list(response)
                        response = json.loads(response[0][2])
                        response_ = list(response)
                        response = response_[1][0]
                        if len(response) == 1:
                            if len(response[0]) > 5:
                                sentences = response[0][5]
                            else: ## only url
                                sentences = response[0][0]
                                if pronounce == False:
                                    return sentences
                                elif pronounce == True:
                                    return [sentences,None,None]
                            translate_text = ""
                            for sentence in sentences:
                                sentence = sentence[0]
                                translate_text += sentence.strip() + ' '
                            translate_text = translate_text
                            if pronounce == False:
                                return translate_text
                            elif pronounce == True:
                                pronounce_src = (response_[0][0])
                                pronounce_tgt = (response_[1][0][0][1])
                                return [translate_text, pronounce_src, pronounce_tgt]
                        elif len(response) == 2:
                            sentences = []
                            for i in response:
                                sentences.append(i[0])
                            if pronounce == False:
                                return sentences
                            elif pronounce == True:
                                pronounce_src = (response_[0][0])
                                pronounce_tgt = (response_[1][0][0][1])
                                return [sentences, pronounce_src, pronounce_tgt]
                    except Exception as e:
                        raise e
            r.raise_for_status()
        except requests.exceptions.ConnectTimeout as e:
            raise e
        except requests.exceptions.HTTPError as e:
            # Request successful, bad response
            raise google_new_transError(tts=self, response=r)
        except requests.exceptions.RequestException as e:
            # Request failed
            raise google_new_transError(tts=self)

    def detect(self, text):
        text = str(text)
        if len(text) >= 5000:
            return log.debug("Warning: Can only detect less than 5000 characters")
        if len(text) == 0:
            return ""
        headers = {
            "Referer": "http://translate.google.{}/".format(self.url_suffix),
            "User-Agent":
                "Mozilla/5.0 (Windows NT 10.0; WOW64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/47.0.2526.106 Safari/537.36",
            "Content-Type": "application/x-www-form-urlencoded;charset=utf-8"
        }
        freq = self._package_rpc(text)
        response = requests.Request(method='POST',
                                    url=self.url,
                                    data=freq,
                                    headers=headers)
        try:
            if self.proxies == None or type(self.proxies) != dict:
                self.proxies = {}
            with requests.Session() as s:
                s.proxies = self.proxies
                r = s.send(request=response.prepare(),
                           verify=False,
                           timeout=self.timeout)

            for line in r.iter_lines(chunk_size=1024):
                decoded_line = line.decode('utf-8')
                if "MkEWBc" in decoded_line:
                    # regex_str = r"\[\[\"wrb.fr\",\"MkEWBc\",\"\[\[(.*).*?,\[\[\["
                    try:
                        # data_got = re.search(regex_str,decoded_line).group(1)
                        response = (decoded_line)
                        response = json.loads(response)
                        response = list(response)
                        response = json.loads(response[0][2])
                        response = list(response)
                        detect_lang = response[0][2]
                    except Exception:
                        raise Exception
                    # data_got = data_got.split('\\\"]')[0]
                    return [detect_lang, LANGUAGES[detect_lang.lower()]]
            r.raise_for_status()
        except requests.exceptions.HTTPError as e:
            # Request successful, bad response
            log.debug(str(e))
            raise google_new_transError(tts=self, response=r)
        except requests.exceptions.RequestException as e:
            # Request failed
            log.debug(str(e))
            raise google_new_transError(tts=self)
#Bismillah

# Core Pkg
import streamlit as st 
import os
os.environ['LI_AT_COOKIE'] = "AQEDATL5DxIEeBgwAAABfNIMGqcAAAF89hiep1YANfMYAQlcYeXsiYLrUBscUe7kPTc2GmoiNqRu9EF8HGXGKFYtbIkVkea4q4mJtKUjQ5ChMqiBxu9zap0-Wkl2WxTXEeQCna7wOmJ45ewT13JH7wQL"

# Load LSA
import gensim
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity,linear_kernel

#Packages Scraping
import logging
from linkedin_jobs_scraper import LinkedinScraper
from linkedin_jobs_scraper.events import Events, EventData
from linkedin_jobs_scraper.query import Query, QueryOptions, QueryFilters
from linkedin_jobs_scraper.filters import RelevanceFilters, TimeFilters, TypeFilters, ExperienceLevelFilters
from google_trans_new import google_translator  
from webdriver_manager.driver import GeckoDriver  
translator = google_translator() 

#Packages Pra-Proses
import nltk
import pandas as pd
import re
import numpy as np
from gensim import corpora, similarities
from gensim.models import LsiModel
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 
stop_words = set(stopwords.words('english'))
print (stop_words)

#package gambar
from PIL import Image

from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait


options = Options()
# options.add_argument("--headless")
# options.add_argument("--no-sandbox")
# options.add_argument("--disable-dev-shm-usage")
# options.add_argument("--disable-gpu")
# options.add_argument("--disable-features=NetworkService")
# options.add_argument("--window-size=1920x1080")
# options.add_argument("--disable-features=VizDisplayCompositor")

# port = int(os.environ.get('PORT',92222))
# options.add_argument(f'--emote-debugging-port={port}')

def get_chromedriver_path():
    results = glob.glob('/**/chromedriver', recursive=True)  # workaround on streamlit sharing
    which = results[0]
    return which



#packages Link
from selenium import webdriver
from webdriver_manager.firefox import GeckoDriverManager

# Load Our Dataset
def load_data(data):
	df = pd.read_csv(data)
	return df 

def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# Security
#passlib,hashlib,bcrypt,scrypt
import hashlib
def make_hashes(password):
	return hashlib.sha256(str.encode(password)).hexdigest()

def check_hashes(password,hashed_text):
	if make_hashes(password) == hashed_text:
		return hashed_text
	return False
# DB Management
import sqlite3 
conn = sqlite3.connect('data.db')
c = conn.cursor()
# DB  Functions
def create_usertable():
	c.execute('CREATE TABLE IF NOT EXISTS userstable(username TEXT,password TEXT)')

def add_userdata(username,password):
	c.execute('INSERT INTO userstable(username,password) VALUES (?,?)',(username,password))
	conn.commit()

def login_user(username,password):
	c.execute('SELECT * FROM userstable WHERE username =? AND password = ?',(username,password))
	data = c.fetchall()
	return data

def view_all_users():
	c.execute('SELECT * FROM userstable')
	data = c.fetchall()
	return data

def main():
	"""Login"""

	st.title("Login")

	menu = ["Home","Login","SignUp","About"]
	choice = st.sidebar.selectbox("Menu",menu)

	if choice == "Home":
		st.subheader("Home")

	elif choice == "Login":
		st.subheader("Login Section")

		username = st.sidebar.text_input("User Name")
		password = st.sidebar.text_input("Password",type='password')
		if st.sidebar.checkbox("Login"):
			# if password == '12345':
			create_usertable()
			hashed_pswd = make_hashes(password)

			result = login_user(username,check_hashes(password,hashed_pswd))
			if result:

				st.success("Logged In as {}".format(username))

				task = st.selectbox("Task",["Recommend","TemplateCV","Profiles"])

				if task == "Recommend":
					st.subheader("Recommend Job")
					st.subheader("Perbarui Iklan Linkedin")
					Negara = st.text_input("Input Negara")		
					job_title = st.text_input("Input Job Title")
					#jum = st.number_input("Input Banyak Iklan yang ingin Ditelusuri")
					jum = st.number_input("Input Banyak Iklan yang ingin Ditelusuri",2,100,5)#mulai,max,default
					if st.button("Perbarui"):
						try:
							# Change root logger level (default is WARN)
							logging.basicConfig(level = logging.INFO)
							id = []
							post_title = []
							company_name = []
							post_date = []
							job_location = []
							job_des = []
							link = []

							def on_data(data: EventData):
								#print('[ON_DATA]', data.title, data.company, data.date, data.description, data.link, len(data.description))
								#post_title.append(translator.translate(data.title, lang_src='auto',lang_tgt='en'))
								post_title.append(data.title)
								id_job = (len(post_title))
								id.append(id_job)
								job_location.append(data.place)
								company_name.append(data.company)
								post_date.append(data.date)
								#job_desc = translator.translate(data.description, lang_src='auto',lang_tgt='en')
								job_des.append(data.description)
								link.append(data.link)					
								
							def on_error(error):
								print('[ON_ERROR]', error)

							def on_end():
								print('[ON_END]')

							scraper = LinkedinScraper(
								chrome_executable_path="chromedriver", # Custom Chrome executable path (e.g. /foo/bar/bin/chromedriver) 
								chrome_options=None,  # Custom Chrome options here
								headless=True,  # Overrides headless mode only if chrome_options is None
								max_workers=1,  # How many threads will be spawned to run queries concurrently (one Chrome driver for each thread)
								slow_mo=1,  # Slow down the scraper to avoid 'Too many requests (429)' errors
							)

							# Add event listeners
							scraper.on(Events.DATA, on_data)
							scraper.on(Events.ERROR, on_error)
							scraper.on(Events.END, on_end)

							queries = [
								Query(
									query=job_title,
									options=QueryOptions(
										#locations=['Indonesia'],
										locations=Negara,
										optimize=False,
										limit=int(jum),
										filters=QueryFilters(
							#                 company_jobs_url='https://www.linkedin.com/jobs/search/?f_C=1441%2C17876832%2C791962%2C2374003%2C18950635%2C16140%2C10440912&geoId=92000000',  # Filter by companies
											relevance=RelevanceFilters.RECENT,
											
							#                 type=[TypeFilters.FULL_TIME, TypeFilters.INTERNSHIP],
							#                 experience=None,
										)
									)
								)
							]

							scraper.run(queries)	
							
							job_data = pd.DataFrame({'Job_ID':id,
							'Date': post_date,
							'Company Name': company_name,
							'Job_Title': post_title,
							'Location': job_location,
							'Description': job_des,
							'Link': link,
													
							})

							# cleaning description column
							job_data['Description'] = job_data['Description'].str.replace('\n',' ')
								
							
							print(job_data.info())
							st.subheader("Data Hasil Scrapppp")
							#job_data.head()
							job_data.to_csv('datascraptest.csv', index=0, encoding='utf-8')
							dframe = load_data("datascraptest.csv")
							st.dataframe(dframe.head(10))
							
						except:
							results= "Not Found"

					st.subheader("Filter Job")
					Negara2 = Negara
					job_title2 = job_title
					#jum = st.number_input("Input Banyak Iklan yang ingin Ditelusuri")
					filter_jobtype = [TypeFilters.FULL_TIME, TypeFilters.PART_TIME, TypeFilters.TEMPORARY, TypeFilters.CONTRACT]
					jobtype = st.selectbox("Job_Type", filter_jobtype)
					filter_time = [TimeFilters.DAY,TimeFilters.WEEK,TimeFilters.MONTH,TimeFilters.ANY]
					time_iklan = st.selectbox("Date Posted",filter_time)
					jum = st.number_input("Input Jumlah Iklan yang ingin Ditelusuri",2,100,5)#mulai,max,default
					if st.button("Perbarui Iklan"):
						try:
							# Change root logger level (default is WARN)
							logging.basicConfig(level = logging.INFO)
							id = []
							post_title = []
							company_name = []
							post_date = []
							job_location = []
							job_des = []
							link = []

							def on_data(data: EventData):
							#     print('[ON_DATA]', data.title, data.company, data.date, data.description, data.link, len(data.description))
								# post_title.append(translator.translate(data.title, lang_src='auto',lang_tgt='en'))
								post_title.append(data.title)
								id_job = (len(post_title))
								id.append(id_job)
								job_location.append(data.place)
								# company_name.append(translator.translate(data.company, lang_src='auto',lang_tgt='en'))
								company_name.append(data.company)
								post_date.append(data.date)
								# job_des.append(translator.translate(data.description, lang_src='auto',lang_tgt='en'))
								job_des.append(data.description)
								link.append(data.link)						
								
							def on_error(error):
								print('[ON_ERROR]', error)

							def on_end():
								print('[ON_END]')

							scraper = LinkedinScraper(
								chrome_executable_path="chromedriver", # Custom Chrome executable path (e.g. /foo/bar/bin/chromedriver) 
								chrome_options=None,  # Custom Chrome options here
								headless=True,  # Overrides headless mode only if chrome_options is None
								max_workers=1,  # How many threads will be spawned to run queries concurrently (one Chrome driver for each thread)
								slow_mo=1,  # Slow down the scraper to avoid 'Too many requests (429)' errors
							)

							# Add event listeners
							scraper.on(Events.DATA, on_data)
							scraper.on(Events.ERROR, on_error)
							scraper.on(Events.END, on_end)

							queries = [
								Query(
									query=job_title,
									options=QueryOptions(
										#locations=['Indonesia'],
										locations=Negara,
										optimize=False,
										limit=int(jum),
										filters=QueryFilters(
							#                 company_jobs_url='https://www.linkedin.com/jobs/search/?f_C=1441%2C17876832%2C791962%2C2374003%2C18950635%2C16140%2C10440912&geoId=92000000',  # Filter by companies
											relevance=RelevanceFilters.RECENT,
											type=jobtype,
											time=time_iklan,
							#                 type=[TypeFilters.FULL_TIME, TypeFilters.INTERNSHIP],
							#                 experience=None,
										)
									)
								)
							]

							scraper.run(queries)	
							
							job_data = pd.DataFrame({'Job_ID':id,
							'Date': post_date,
							'Company Name': company_name,
							'Job_Title': post_title,
							'Location': job_location,
							'Description': job_des,
							'Link': link,
													
							})

							# cleaning description column
							job_data['Description'] = job_data['Description'].str.replace('\n',' ')
								
							
							#print(job_data.info())
							st.subheader("Data Hasil Scrap")
							#job_data.head()
							job_data.to_csv('datascraptest.csv', index=0, encoding='utf-8')
							dframe = load_data("datascraptest.csv")
							st.dataframe(dframe.head(10))
							
						except:
							results= "Not Found"	


					st.subheader("Upload CV untuk Memukan Rekomendasi Iklan Pekerjaan")
					file = st.file_uploader("", type='csv')
					jumlah = st.number_input("Input Banyak Iklan yang ingin Ditampilkan",2,100,10)#mulai,max,default
					if st.button("Temukan Iklan yang Cocok"):
						try:
							def remove_punc(text):
								symbols = r"â€¢!\"#$%&()*+-.,/:;<=>?@[\]^_`'{|}~\\0123456789" 
								output = [char for char in text if char not in symbols]
								return "".join(output)

							def stemSentence(tokens):
							#token_words=word_tokenize(text)
								
								porter = PorterStemmer()
								stem_sentence=[]
								for word in tokens:
									stem_sentence.append(porter.stem(word))
								return stem_sentence

							def lemmatization(tokens):
								lemmatizer = WordNetLemmatizer() 
								return [lemmatizer.lemmatize(word) for word in tokens ]

							def stopwordSentence(tokens):
								return [word for word in tokens if word not in stop_words]

							def caseFold(text):
								return text.lower()

							def preProcessPipeline(text, print_output=False):
								if print_output:
									print('Teks awal:')
									print(text)
								text = remove_punc(text)
								if print_output:
									print('Setelah menghilangkan tanda baca:')
									print(text)

								text = caseFold(text)
								if print_output:
									print('Setelah Casefold')
									print(text)

								token_words = word_tokenize(text)    
								token_words = lemmatization(token_words)
								if print_output:
									print("Setelah lemmatization:")
									print(" ".join(token_words))

								token_words = stopwordSentence(token_words)
								if print_output:
									print("Setelah menghilangkan stopwords:")
									print(" ".join(token_words))

								return " ".join(token_words)
							documents_train = pd.read_csv('datascraptest.csv', error_bad_lines=False)
							train_text = documents_train['Description'].apply(preProcessPipeline)
							documents_test = pd.read_csv(file, error_bad_lines=False)
							test_text = documents_test['cv_desc'].apply(preProcessPipeline)

							nltk_tokens = [nltk.word_tokenize(i) for i in train_text]
							y = nltk_tokens

							dictionary = gensim.corpora.Dictionary(y)
							text = y

							corpus = [dictionary.doc2bow(i) for i in text]
							tfidf = gensim.models.TfidfModel(corpus, smartirs='npu')
							corpus_tfidf = tfidf[corpus]

							lsi_model = LsiModel(corpus=corpus_tfidf,id2word=dictionary, num_topics = 3)
							print("Derivation of Term Matrix T of Training Document Word Stems: ",lsi_model.get_topics())
								#Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]
							print("LSI Vectors of Training Document Word Stems: ",[lsi_model[document_word_stems] for document_word_stems in corpus])
							cosine_similarity_matrix = similarities.MatrixSimilarity(lsi_model[corpus])


							word_tokenizer = nltk.tokenize.WordPunctTokenizer()
							words = word_tokenizer.tokenize(test_text[0])

							vector_lsi_test = lsi_model[dictionary.doc2bow(words)]

							cosine_similarities_test = cosine_similarity_matrix[vector_lsi_test]

							most_similar_document_test = train_text[np.argmax(cosine_similarities_test)]

							cst = (cosine_similarities_test)
							
							cst_terurut = sorted(cosine_similarities_test, reverse=True)

							iklan = cosine_similarities_test.argsort()[-jumlah:][::-1]
							def generator_cosines(iklan):
								for i in iklan:
									yield i						
							#print data awal di csv 
							for i in iklan:	
									st.write("Post Date :",f"{documents_train['Date'][i]}\n" )
									st.write("Nama Perusahaan :",f"{documents_train['Company Name'][i]}\n" )
									st.write("Nama Pekerjaan :",f"{documents_train['Job_Title'][i]}\n" )
									st.write("Deskripsi Pekerjaan :",f"{documents_train['Description'][i]}\n" )
									st.write("Negara :",f"{documents_train['Location'][i]}\n" )
									st.write("Link Iklan pekerjaan :",f"{documents_train['Link'][i]}\n" )
									st.write("Cosine similiarity cv terhadap iklan:", f"{cst[i]}")
									
									st.subheader("-----------------------------------------------------------------------------------")

						except:
							results= "Not Found"
					
				elif task == "TemplateCV":
						st.subheader("Download Template CV ")	
						st.write('Klik button untuk melakukan download')
						#st.write('https://drive.google.com/file/d/1LUyxJgdXEQdPMTuqSCdNjdIKqVjE7z80/view?usp=sharing')
						with open("cvit.csv", "rb") as file:
							btn = st.download_button(
							      label="Download",
							      data=file,
							      file_name="template.csv",
							      mime="text/csv"
							    )
						st.subheader("Panduan Penulisan CV sesuai Template")	
						image = Image.open('kotaklogo.png')
						st.image(image, caption='Format CV')
	
						
				elif task == "Profiles":
					st.subheader("User Profiles")
					user_result = view_all_users()
					clean_db = pd.DataFrame(user_result,columns=["Username","Password"])
					st.dataframe(clean_db)
		else:
			st.warning("Incorrect Username/Password")

	elif choice == "SignUp":
		st.subheader("Create New Account")
		new_user = st.text_input("Username")
		new_password = st.text_input("Password",type='password')

		if st.button("Signup"):
			create_usertable()
			add_userdata(new_user,make_hashes(new_password))
			st.success("You have successfully created a valid Account")
			st.info("Go to Login Menu to login")

	elif choice == "About":
		st.subheader("About")
		st.text("Built with Streamlit & Pandas")


if __name__ == '__main__':
	main()